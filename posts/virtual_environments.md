# Virtual Environments in Data
*by [Iaroslav Zeigerman](https://github.com/izeigerman)*

We start this post by setting context for what environments are and how they are used in software today. If you want to skip this part and jump right in please proceed to the [Environments in Data](#environments-in-data) section.

## Environments in Software
Environments have always been a way for software engineers to test and promote their changes in a safe manner, reducing this way the possibility of causing regressions in production. Generally each environment represents a set of isolated resources to which the software in question is deployed.

Over time the industry settled on a certain set of environments that is now considered to be the best practice:
- **Testing** - used for testing the bleeding edge of the target software. This environment is updated frequently with most recent code changes. Deployments to this environment are low risk since it's only used by developers of a specific team and is never exposed to end users.
- **Staging** - used for testing changes that are about to be released and promoted to production. Deployment to this environment happens shortly before the release and is usually only updated with critical bug fixes until the release takes place. Deployments to this environment are of a higher risk since it can be shared across multiple teams and/or organizations and can even be sometimes exposed to a limited set of partners and/or consumers.
- **Production** - the name speaks for itself. Only released changes are promoted to this environment, usually after going through extensive testing and validation on staging. Deployments to this environment are of highest risk since it's the one exposed to consumers.

These environments, however, used to be quite rigid. The process of provisioning new environments or updating existing ones used to take a lot of time and required a fair bit of coordination across teams or even within a single team. Long story short, the process hardly scaled.

This changed with the rise of Docker and CI/CD which made the process of spinning up new testing environments extremely cheap and easy. This, in turn, resulted in a significant boost of productivity for individual developers, who could now easily provision new testing environments specifically for their needs and then tear them down as easily once they no longer needed them. Similarly, promoting tested changes to staging and production became equally straightforward.

## Environments in Data
Environments have been routinely used by practitioners from the data analysis and data engineering fields. Tools like **dbt** allow its users to configure different *targets* - connections to different schemas within a Data Warehouse or even separate Data Warehouses instances altogether. This mechanism is sufficient to enable analysts and engineers to deploy their data pipelines into separate environments for testing, staging and production, while ensuring that changes are only applied to the target environment keeping tables in other environments unaffected. All such environments are usually provisioned once and then shared between all users who use the tool.

![Figure 1: Isolated but rigid Data Warehouse environments](fixme.png)
*Figure 1: Isolated but rigid Data Warehouse environments.*

Needless to say that such a development process becomes quite troublesome once the organization grows in size and multiple users begin to push changes, oftentimes concurrently. Some of the challenges include:
- Users deploy and run their data pipelines in shared testing environments, running the risk of overriding each others' changes which leads to unexpected results and hours wasted on troubleshooting.
- Populating different environments with data is hard. Environments other than production usually contain either a small subset of production data or randomly generated values. Neither is sufficient to get an adequate preview of the impact the changes would have when applied to production data.
- When pipeline changes are promoted to production they are not reflected on underlying datasets immediately, which means there exists a period of inconsistency between the code and data generated by it.

Somehow the analogue of the Docker + CI/CD revolution never happened in the data processing field.

Until now, that is.

In this post we introduce the concept of **virtual environments** in SQLMesh and demonstrate how it helps users realize the following benefits:
- Creation of safe and isolated testing environments is fast and comes at almost no cost.
- New environments are populated with representative data right away. No action from a user or extensive data copying is needed.
- Multiple versions of the same datasets coexist at the same time, making it a trivial operation to roll changes back / forward.
- Changes promoted to production are reflected on underlying datasets immediately. Therefore production data and the code that generates it are always in sync.

## Introducing Virtual Environments
When it comes to spinning up new data environments cheaply and easily, the primary challenge becomes populating such new environments with representative data. By "representative" we mean data necessary to preview and adequately assess the impact the proposed changes would have on production datasets. Thus it's only too reasonable to conclude that the most representative datasets are the production ones.

Therefore we can define the problem statement as "how can we use production datasets in a testing environment while ensuring that no changes applied to such an environment impact any existing datasets in all other environments". More generally, we're looking for a way to **reuse** existing data when appropriate to get an accurate **preview** of proposed changes in a fully **isolated** manner.

In SQLMesh we achieve exactly this thanks to the following aspects of the platform:
- Each dataset managed by the platform is populated by a logic defined as a [model](https://sqlmesh.readthedocs.io/en/stable/concepts/models/overview/) using either SQL or Python. Every time a change to an existing model is made a new [snapshot](https://sqlmesh.readthedocs.io/en/stable/concepts/architecture/snapshots/) of this model gets created and associated with a unique [fingerprint](https://sqlmesh.readthedocs.io/en/stable/concepts/architecture/snapshots/#fingerprinting). The fingerprint itself is a combination of hashes computed using attributes that constitute a model. By default each model snapshot writes into its own unique table (or updates its own unique view), which means that multiple versions of a model can coexist at the same time without causing conflicts.
- The platform doesn't expose datasets (tables or views) populated with model snapshots directly. Instead it provides access to them through the layer of indirection implemented using [views](https://en.wikipedia.org/wiki/View_(SQL)). This way updating a dataset version becomes an atomic and almost instantaneous operation of updating a view associated with it by swapping the source it points to. The best part is that this operation is completely transparent to downstream consumers who always refer to a view and never to an underlying table. We refer to the layer of indirection powered by views as the **virtual layer**, while the layer of tables and views populated directly with model snapshots is called the **physical layer**.

![Figure 2: Separation of Virtual and Physical layers](fixme.png)
*Figure 2: Separation of Virtual and Physical layers.*

These two properties combined is what constitutes **virtual environments**.

### Model Snapshots
As mentioned earlier, every time a model changes a new model snapshot gets created to capture the change. Each snapshot represents the state of a model at the time when this snapshot was generated. In SQLMesh snapshots are generated automatically when a new [plan](https://sqlmesh.readthedocs.io/en/stable/concepts/plans/) is created and applied.

Each snapshot is uniquely identified by what we refer to as its "fingerprint". A fingerprint consists of the following attributes:
- The data hash of the model is based on model attributes that have direct impact on the dataset produced by this model. Such attributes include the model's query, storage format, partitioning scheme, etc. When this hash changes we say that the model was modified **directly**.
- The data hash of upstream models. This is the same data hash calculation logic but applied to models that are upstream from the current model in order to capture data changes caused by the model's dependencies. Similarly, when this hash changes we say that the model was modified **indirectly**.
- The metadata hash of the model is based on model attributes that have no impact on data itself. This includes metadata like ownership information, descriptions, comments, etc.
- The metadata hash of upstream models captures metadata changes in model's dependencies.

The difference between **direct** and **indirect** modifications becomes relevant when [categorizing](https://sqlmesh.readthedocs.io/en/stable/concepts/plans/#change-categories) individual changes during plan creation.

When a model is modified directly (i.e. its data hash changes), the snapshot which captures the change gets associated with its own unique table (or a view) in the **physical layer**, the name of which is based on the snapshot's fingerprint. Thus different versions of the same model can coexist and be evaluated at the same time without overriding each others' outputs. The platform keeps track of missing data intervals for each individual snapshot and automatically [backfills](https://sqlmesh.readthedocs.io/en/stable/concepts/plans/#backfilling) existing data gaps during the plan application.

Model snapshots alone are not enough, however, since we still need to somehow expose various dataset versions in different environments as well as be able to swap these versions on the fly without requiring changes to existing workflows. We address both of these items in the **virtual layer**.

### Virtual Layer
When a downstream consumer or a workflow wants to access data produced by a model they do this by querying a view that is part of the **virtual layer**. Datasets generated by model snapshots are never accessed directly which allows SQLMesh update these views to point to different dataset versions without affecting downstream. Sort of like using a pointer or a reference in programming.

This idea is not new. Storage engines like [Iceberg](https://iceberg.apache.org/) and [Delta Lake](https://delta.io/) implement something quite similar in the storage layer, which lets their users maintain multiple versions (a.k.a. snapshots) of the same dataset and easily travel back in time when needed.

What's new, however, is how SQLMesh uses this technique to manage isolated environments. Each [environment](https://sqlmesh.readthedocs.io/en/stable/concepts/environments/) in SQLMesh is just a collection of views, one per model, each pointing at one of the snapshot tables in the **physical layer**.

![Figure 3: Environments managed through the virtual layer](fixme.png)
*Figure 3: Environments managed through the virtual layer.*

Views that belong to environments other than the production one get an environment name attached as a suffix to the schema portion of their fully qualified names. Thus the `db.model_a` dataset can be accessed using its original name in the production environment, while in order to access it in the environment named `test` the `db__test.model_a` name should be used instead. That's how changes can be **previewed** before making their way into production.

When a new environment is created it shares the same set of snapshot table pointers with the production environment. Once changes are applied to that environment, new model snapshots are generated and some of the views get updated. Views that are part of the production environment remain unaffected. This is how SQLMesh ensures environment **isolation**.

Finally, if a model implementation hasn't changed in relation to production (either **directly** or **indirectly**), the platform can safely **reuse** the same snapshot table that is currently being used in the production environment.

## Conclusions
With **virtual environments** SQLMesh is able to provide fully **isolated** testing environments with a complete **preview** of changes before they make it into production while **reusing** production datasets when it's appropriate and safe to do so. This, in turn, results in very tangible benefits for end users:
- Creating new environments is cheap since it only involves creation of a new set of views.
- Data is immediately available in new environments thanks to the separation between **virtual** and **physical** layers.
- Rolling back a change happens almost instantaneously since no data movement is involved and only views that are part of the **virtual layer** get updated.
- Promoting changes to production is also a **virtual layer** operation, which helps ensure that data and code are always in sync.

When it comes to the last point of promoting changes to production, we are about to release the [CI/CD bot](https://github.com/TobikoData/sqlmesh/blob/main/docs/integrations/github.md) which will help streamline and automate this process. Don't miss out, join our [Slack channel](https://join.slack.com/t/tobiko-data/shared_invite/zt-1ma66d79v-a4dbf4DUpLAQJ8ptQrJygg) and stay tuned.
