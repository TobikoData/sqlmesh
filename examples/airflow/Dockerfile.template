FROM apache/spark:3.5.0-python3 AS spark

FROM apache/airflow:$AIRFLOW_VERSION-python3.8

USER root

# Fix the airflow user UID
ENV AIRFLOW_UID=$AIRFLOW_UID
RUN usermod -u $AIRFLOW_UID airflow

# Workaround the expired MySQL GPG key.
RUN rm -f /etc/apt/sources.list.d/mysql.list

RUN apt-get autoclean
RUN apt-get update

# Install system packages
RUN apt install -y default-jdk gcc g++ make git

ENV JAVA_HOME="/usr/lib/jvm/default-java/"

# Install Spark
COPY --from=spark /opt/spark /opt/spark
RUN chown -R airflow /opt/spark
ENV SPARK_HOME="/opt/spark"
ENV PATH="$PATH:$SPARK_HOME/bin"

# Install Postgres driver and Iceberg for Spark
RUN curl https://jdbc.postgresql.org/download/postgresql-42.5.0.jar -o /opt/spark/jars/postgresql-42.5.0.jar && \
    curl -L https://search.maven.org/remotecontent?filepath=org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.5.1/iceberg-spark-runtime-3.5_2.12-1.5.1.jar -o /opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.1.jar

# Install Hadoop
RUN curl https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz -o hadoop-3.3.6.tar.gz && \
    tar xf hadoop-3.3.6.tar.gz -C /opt/ && \
    mv /opt/hadoop-3.3.6 /opt/hadoop

ENV HADOOP_HOME="/opt/hadoop"

# Install Hive
RUN curl https://dlcdn.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz -o apache-hive-3.1.3-bin.tar.gz && \
    tar xf apache-hive-3.1.3-bin.tar.gz -C /opt/ && \
    mv /opt/apache-hive-3.1.3-bin /opt/hive

ENV HIVE_HOME="/opt/hive"

# Airflow connections
ENV AIRFLOW_CONN_SPARK_DEFAULT="spark://local?deploy-mode=client"

# Airflow configuration
ENV AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=3

# SQLMesh configuration
ENV SQLMESH__DISABLE_ANONYMIZED_ANALYTICS=1

USER airflow

# Install Spark provider for Airflow
# skip install pyspark since it's part of the image
RUN pip install apache-airflow-providers-apache-spark==4.8.0 --no-deps
RUN pip install apache-airflow-providers-databricks==6.4.0 \
                apache-airflow-providers-github==2.6.0 \
                apache-airflow-providers-common-sql==1.13.0 \
                pandas==1.5.2  # python 3.8 spark 3.4 and pandas 2.0 have issues with casting timestamp

# Install Deps
USER root
ADD setup.py /opt/sqlmesh/setup.py
RUN mkdir /opt/sqlmesh/sqlmesh && touch /opt/sqlmesh/sqlmesh/__init__.py
RUN chown -R airflow /opt/sqlmesh
USER airflow
RUN cd /opt/sqlmesh && pip install -e .[dbt]
